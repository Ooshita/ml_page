{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noriakioshita/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-b8e886ceae22>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/noriakioshita/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Users/noriakioshita/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/noriakioshita/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/noriakioshita/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/noriakioshita/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "INFO:tensorflow:Summary name validation error is illegal; using validation_error instead.\n",
      "WARNING:tensorflow:Passing a `GraphDef` to the SummaryWriter is deprecated. Pass a `Graph` object instead, such as `sess.graph`.\n",
      "Epoch: 0001 cost: 1.176469925\n",
      "Validation Error: 0.14880001544952393\n",
      "Epoch: 0002 cost: 0.662541118\n",
      "Validation Error: 0.12959998846054077\n",
      "Epoch: 0003 cost: 0.550731546\n",
      "Validation Error: 0.11919999122619629\n",
      "Epoch: 0004 cost: 0.496875220\n",
      "Validation Error: 0.11339998245239258\n",
      "Epoch: 0005 cost: 0.463759620\n",
      "Validation Error: 0.10939997434616089\n",
      "Epoch: 0006 cost: 0.440912680\n",
      "Validation Error: 0.10600000619888306\n",
      "Epoch: 0007 cost: 0.423913988\n",
      "Validation Error: 0.10540002584457397\n",
      "Epoch: 0008 cost: 0.410653060\n",
      "Validation Error: 0.1029999852180481\n",
      "Epoch: 0009 cost: 0.399901470\n",
      "Validation Error: 0.10100001096725464\n",
      "Epoch: 0010 cost: 0.390948518\n",
      "Validation Error: 0.09820002317428589\n",
      "Epoch: 0011 cost: 0.383341761\n",
      "Validation Error: 0.09859997034072876\n",
      "Epoch: 0012 cost: 0.376733968\n",
      "Validation Error: 0.09520000219345093\n",
      "Epoch: 0013 cost: 0.371069684\n",
      "Validation Error: 0.09399998188018799\n",
      "Epoch: 0014 cost: 0.365907887\n",
      "Validation Error: 0.0942000150680542\n",
      "Epoch: 0015 cost: 0.361399271\n",
      "Validation Error: 0.09119999408721924\n",
      "Epoch: 0016 cost: 0.357263960\n",
      "Validation Error: 0.09020000696182251\n",
      "Epoch: 0017 cost: 0.353486442\n",
      "Validation Error: 0.08960002660751343\n",
      "Epoch: 0018 cost: 0.350161089\n",
      "Validation Error: 0.0899999737739563\n",
      "Epoch: 0019 cost: 0.346996113\n",
      "Validation Error: 0.08960002660751343\n",
      "Epoch: 0020 cost: 0.344149518\n",
      "Validation Error: 0.0878000259399414\n",
      "Epoch: 0021 cost: 0.341430372\n",
      "Validation Error: 0.08840000629425049\n",
      "Epoch: 0022 cost: 0.338988202\n",
      "Validation Error: 0.08700001239776611\n",
      "Epoch: 0023 cost: 0.336682706\n",
      "Validation Error: 0.08600002527236938\n",
      "Epoch: 0024 cost: 0.334487021\n",
      "Validation Error: 0.08579999208450317\n",
      "Epoch: 0025 cost: 0.332461779\n",
      "Validation Error: 0.08579999208450317\n",
      "Epoch: 0026 cost: 0.330537619\n",
      "Validation Error: 0.08579999208450317\n",
      "Epoch: 0027 cost: 0.328708540\n",
      "Validation Error: 0.08499997854232788\n",
      "Epoch: 0028 cost: 0.327015461\n",
      "Validation Error: 0.08480000495910645\n",
      "Epoch: 0029 cost: 0.325406449\n",
      "Validation Error: 0.08399999141693115\n",
      "Epoch: 0030 cost: 0.323843831\n",
      "Validation Error: 0.08380001783370972\n",
      "Epoch: 0031 cost: 0.322395465\n",
      "Validation Error: 0.08459997177124023\n",
      "Epoch: 0032 cost: 0.320962412\n",
      "Validation Error: 0.08279997110366821\n",
      "Epoch: 0033 cost: 0.319611573\n",
      "Validation Error: 0.08279997110366821\n",
      "Epoch: 0034 cost: 0.318369059\n",
      "Validation Error: 0.08340001106262207\n",
      "Epoch: 0035 cost: 0.317132466\n",
      "Validation Error: 0.08160001039505005\n",
      "Epoch: 0036 cost: 0.315962788\n",
      "Validation Error: 0.08319997787475586\n",
      "Epoch: 0037 cost: 0.314819619\n",
      "Validation Error: 0.08279997110366821\n",
      "Epoch: 0038 cost: 0.313725259\n",
      "Validation Error: 0.08279997110366821\n",
      "Epoch: 0039 cost: 0.312677211\n",
      "Validation Error: 0.08139997720718384\n",
      "Epoch: 0040 cost: 0.311667407\n",
      "Validation Error: 0.0812000036239624\n",
      "Epoch: 0041 cost: 0.310760242\n",
      "Validation Error: 0.08160001039505005\n",
      "Epoch: 0042 cost: 0.309816561\n",
      "Validation Error: 0.08079999685287476\n",
      "Epoch: 0043 cost: 0.308866583\n",
      "Validation Error: 0.0812000036239624\n",
      "Epoch: 0044 cost: 0.308006240\n",
      "Validation Error: 0.07999998331069946\n",
      "Epoch: 0045 cost: 0.307183540\n",
      "Validation Error: 0.07999998331069946\n",
      "Epoch: 0046 cost: 0.306351166\n",
      "Validation Error: 0.0788000226020813\n",
      "Epoch: 0047 cost: 0.305585802\n",
      "Validation Error: 0.07899999618530273\n",
      "Epoch: 0048 cost: 0.304824672\n",
      "Validation Error: 0.07859998941421509\n",
      "Epoch: 0049 cost: 0.304067048\n",
      "Validation Error: 0.07899999618530273\n",
      "Epoch: 0050 cost: 0.303347463\n",
      "Validation Error: 0.07899999618530273\n",
      "Epoch: 0051 cost: 0.302652113\n",
      "Validation Error: 0.0788000226020813\n",
      "Epoch: 0052 cost: 0.301972993\n",
      "Validation Error: 0.07740002870559692\n",
      "Epoch: 0053 cost: 0.301330657\n",
      "Validation Error: 0.07819998264312744\n",
      "Epoch: 0054 cost: 0.300636387\n",
      "Validation Error: 0.07819998264312744\n",
      "Epoch: 0055 cost: 0.300055789\n",
      "Validation Error: 0.07760000228881836\n",
      "Epoch: 0056 cost: 0.299456178\n",
      "Validation Error: 0.078000009059906\n",
      "Epoch: 0057 cost: 0.298830236\n",
      "Validation Error: 0.07840001583099365\n",
      "Epoch: 0058 cost: 0.298275220\n",
      "Validation Error: 0.07740002870559692\n",
      "Epoch: 0059 cost: 0.297698789\n",
      "Validation Error: 0.07719999551773071\n",
      "Epoch: 0060 cost: 0.297158033\n",
      "Validation Error: 0.07740002870559692\n",
      "Epoch: 0061 cost: 0.296603897\n",
      "Validation Error: 0.07760000228881836\n",
      "Epoch: 0062 cost: 0.296101145\n",
      "Validation Error: 0.07740002870559692\n",
      "Epoch: 0063 cost: 0.295577774\n",
      "Validation Error: 0.07700002193450928\n",
      "Epoch: 0064 cost: 0.295034947\n",
      "Validation Error: 0.07760000228881836\n",
      "Epoch: 0065 cost: 0.294604574\n",
      "Validation Error: 0.07660001516342163\n",
      "Epoch: 0066 cost: 0.294122442\n",
      "Validation Error: 0.07639998197555542\n",
      "Epoch: 0067 cost: 0.293597684\n",
      "Validation Error: 0.07580000162124634\n",
      "Epoch: 0068 cost: 0.293158079\n",
      "Validation Error: 0.07660001516342163\n",
      "Epoch: 0069 cost: 0.292686147\n",
      "Validation Error: 0.07599997520446777\n",
      "Epoch: 0070 cost: 0.292280551\n",
      "Validation Error: 0.07580000162124634\n",
      "Epoch: 0071 cost: 0.291845801\n",
      "Validation Error: 0.07620000839233398\n",
      "Epoch: 0072 cost: 0.291404564\n",
      "Validation Error: 0.07639998197555542\n",
      "Epoch: 0073 cost: 0.290956148\n",
      "Validation Error: 0.07660001516342163\n",
      "Epoch: 0074 cost: 0.290611436\n",
      "Validation Error: 0.0756000280380249\n",
      "Epoch: 0075 cost: 0.290209026\n",
      "Validation Error: 0.07580000162124634\n",
      "Epoch: 0076 cost: 0.289763435\n",
      "Validation Error: 0.07499998807907104\n",
      "Epoch: 0077 cost: 0.289434506\n",
      "Validation Error: 0.07539999485015869\n",
      "Epoch: 0078 cost: 0.288996286\n",
      "Validation Error: 0.07620000839233398\n",
      "Epoch: 0079 cost: 0.288651002\n",
      "Validation Error: 0.07520002126693726\n",
      "Epoch: 0080 cost: 0.288317505\n",
      "Validation Error: 0.07520002126693726\n",
      "Epoch: 0081 cost: 0.287970621\n",
      "Validation Error: 0.0745999813079834\n",
      "Epoch: 0082 cost: 0.287584462\n",
      "Validation Error: 0.07480001449584961\n",
      "Epoch: 0083 cost: 0.287268638\n",
      "Validation Error: 0.07499998807907104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0084 cost: 0.286934813\n",
      "Validation Error: 0.07480001449584961\n",
      "Epoch: 0085 cost: 0.286580467\n",
      "Validation Error: 0.07499998807907104\n",
      "Epoch: 0086 cost: 0.286269501\n",
      "Validation Error: 0.0745999813079834\n",
      "Epoch: 0087 cost: 0.285940931\n",
      "Validation Error: 0.07520002126693726\n",
      "Epoch: 0088 cost: 0.285603998\n",
      "Validation Error: 0.07499998807907104\n",
      "Epoch: 0089 cost: 0.285338535\n",
      "Validation Error: 0.07520002126693726\n",
      "Epoch: 0090 cost: 0.284973814\n",
      "Validation Error: 0.07499998807907104\n",
      "Epoch: 0091 cost: 0.284654503\n",
      "Validation Error: 0.07539999485015869\n",
      "Epoch: 0092 cost: 0.284407182\n",
      "Validation Error: 0.07520002126693726\n",
      "Epoch: 0093 cost: 0.284115452\n",
      "Validation Error: 0.07480001449584961\n",
      "Epoch: 0094 cost: 0.283805594\n",
      "Validation Error: 0.07520002126693726\n",
      "Epoch: 0095 cost: 0.283560946\n",
      "Validation Error: 0.07480001449584961\n",
      "Epoch: 0096 cost: 0.283246749\n",
      "Validation Error: 0.07520002126693726\n",
      "Epoch: 0097 cost: 0.282943674\n",
      "Validation Error: 0.07480001449584961\n",
      "Epoch: 0098 cost: 0.282719241\n",
      "Validation Error: 0.07539999485015869\n",
      "Epoch: 0099 cost: 0.282443843\n",
      "Validation Error: 0.07499998807907104\n",
      "Epoch: 0100 cost: 0.282168880\n",
      "Validation Error: 0.07499998807907104\n",
      "Optimization Finished!\n",
      "Test Accuracy: 0.921999990940094\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import time, shutil, os\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"data\", one_hot=True)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 100\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "\n",
    "def inference(x):\n",
    "    init = tf.constant_initializer(value=0)\n",
    "    W = tf.get_variable(\"W\", [784, 10], initializer=init)\n",
    "    b = tf.get_variable(\"b\", [10], initializer=init)\n",
    "    output = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "    tf.summary.histogram(\"weights\", W)\n",
    "    tf.summary.histogram(\"biases\", b)\n",
    "    tf.summary.histogram(\"output\", output)\n",
    "    return output\n",
    "\n",
    "\n",
    "def loss(output, y):\n",
    "    dot_product = y * tf.log(output)\n",
    "    # Reduction along axis 0 collapses each column into a single\n",
    "    # value, whereas reduction along axis 1 collapses each row \n",
    "    # into a single value. In general, reduction along axis i \n",
    "    # collapses the ith dimension of a tensor to size 1.\n",
    "    xentropy = -tf.reduce_sum(dot_product, axis=1)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def training(cost, global_step):\n",
    "    tf.summary.scalar(\"cost\", cost)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(cost, global_step=global_step)\n",
    "    return train_op\n",
    "\n",
    "\n",
    "def evaluate(output, y):\n",
    "    correct_prediction = tf.equal(tf.argmax(output, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.scalar(\"validation error\", (1.0 - accuracy))\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Remove old summaries and checkpoints\n",
    "    if os.path.exists(\"logistic_logs\"):\n",
    "        shutil.rmtree(\"logistic_logs\")\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        x = tf.placeholder(\"float\", [None, 784]) # mnist data image of shape 28*28=784\n",
    "        y = tf.placeholder(\"float\", [None, 10]) # 0-9 digits recognition => 10 classes\n",
    "        output = inference(x)\n",
    "        cost = loss(output, y)\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        train_op = training(cost, global_step)\n",
    "        eval_op = evaluate(output, y)\n",
    "        summary_op = tf.summary.merge_all()\n",
    "        saver = tf.train.Saver()\n",
    "        sess = tf.Session()\n",
    "        summary_writer = tf.summary.FileWriter(\n",
    "            \"logistic_logs\",\n",
    "            graph_def=sess.graph_def\n",
    "        )\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        sess.run(init_op)\n",
    "\n",
    "        # Training cycle\n",
    "        for epoch in range(training_epochs):\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(mnist.train.num_examples/batch_size)\n",
    "            # Loop over all batches\n",
    "            for i in range(total_batch):\n",
    "                minibatch_x, minibatch_y = mnist.train.next_batch(batch_size)\n",
    "                # Fit training using batch data\n",
    "                sess.run(train_op, feed_dict={x: minibatch_x, y: minibatch_y})\n",
    "                # Compute average loss\n",
    "                avg_cost += sess.run(\n",
    "                    cost, feed_dict={x: minibatch_x, y: minibatch_y}\n",
    "                ) / total_batch\n",
    "            # Display logs per epoch step\n",
    "            if epoch % display_step == 0:\n",
    "                print(\"Epoch: {:04d} cost: {:.9f}\".format(epoch+1, avg_cost))\n",
    "                accuracy = sess.run(eval_op, feed_dict={x: mnist.validation.images, y: mnist.validation.labels})\n",
    "                print(\"Validation Error: {}\".format(1 - accuracy))\n",
    "                summary_str = sess.run(summary_op, feed_dict={x: minibatch_x, y: minibatch_y})\n",
    "                summary_writer.add_summary(summary_str, sess.run(global_step))\n",
    "                saver.save(sess, os.path.join(\"logistic_logs\", \"model-checkpoint\"), global_step=global_step)\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "        accuracy = sess.run(eval_op, feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
    "        print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Variable mlp_model/hidden_1/W already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-2-9970c8e512bd>\", line 14, in layer\n    W = tf.get_variable(\"W\", weight_shape, initializer=weight_init)\n  File \"<ipython-input-2-9970c8e512bd>\", line 21, in inference\n    hidden_1 = layer(x, [784, n_hidden_1], [n_hidden_1])\n  File \"<ipython-input-2-9970c8e512bd>\", line 42, in <module>\n    output_opt = inference(x)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9970c8e512bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mlp_model\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0moutput_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mcost_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-9970c8e512bd>\u001b[0m in \u001b[0;36minference\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hidden_1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mhidden_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_hidden_1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mn_hidden_1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hidden_2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mhidden_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mn_hidden_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_hidden_2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mn_hidden_2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-9970c8e512bd>\u001b[0m in \u001b[0;36mlayer\u001b[0;34m(input, weight_shape, bias_shape)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mweight_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_normal_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstddev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mweight_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mbias_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"W\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"b\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1315\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1317\u001b[0;31m       constraint=constraint)\n\u001b[0m\u001b[1;32m   1318\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1319\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1077\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m           \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m    423\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m           \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    392\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m           use_resource=use_resource, constraint=constraint)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    731\u001b[0m                          \u001b[0;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 733\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    734\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable mlp_model/hidden_1/W already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-2-9970c8e512bd>\", line 14, in layer\n    W = tf.get_variable(\"W\", weight_shape, initializer=weight_init)\n  File \"<ipython-input-2-9970c8e512bd>\", line 21, in inference\n    hidden_1 = layer(x, [784, n_hidden_1], [n_hidden_1])\n  File \"<ipython-input-2-9970c8e512bd>\", line 42, in <module>\n    output_opt = inference(x)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "n_hidden_1 = 256\n",
    "n_hidden_2 = 256\n",
    "\n",
    "\n",
    "def layer(input, weight_shape, bias_shape):\n",
    "    weight_init = tf.random_normal_initializer(stddev=(2.0/weight_shape[0])**0.5)\n",
    "    bias_init = tf.constant_initializer(value=0)\n",
    "    W = tf.get_variable(\"W\", weight_shape, initializer=weight_init)\n",
    "    b = tf.get_variable(\"b\", bias_shape, initializer=bias_init)\n",
    "    return tf.nn.relu(tf.matmul(input, W) + b)\n",
    "\n",
    "\n",
    "def inference(x):\n",
    "    with tf.variable_scope(\"hidden_1\"):\n",
    "        hidden_1 = layer(x, [784, n_hidden_1], [n_hidden_1])     \n",
    "    with tf.variable_scope(\"hidden_2\"):\n",
    "        hidden_2 = layer(hidden_1, [n_hidden_1, n_hidden_2], [n_hidden_2])\n",
    "    with tf.variable_scope(\"output\"):\n",
    "        output = layer(hidden_2, [n_hidden_2, 10], [10])\n",
    "    return output\n",
    "\n",
    "\n",
    "def loss(output, y):\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=y)    \n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    return loss\n",
    "\n",
    "\n",
    "mnist = input_data.read_data_sets(\"data\", one_hot=True)\n",
    "\n",
    "x = tf.placeholder(\"float\", [None, 784])\n",
    "y = tf.placeholder(\"float\", [None, 10])\n",
    "sess = tf.Session()\n",
    "\n",
    "with tf.variable_scope(\"mlp_model\") as scope:\n",
    "    output_opt = inference(x)\n",
    "    cost_opt = loss(output_opt, y)\n",
    "    saver = tf.train.Saver()\n",
    "    scope.reuse_variables()\n",
    "    var_list_opt = [\n",
    "        \"hidden_1/W\", \"hidden_1/b\",\n",
    "\t\t\"hidden_2/W\", \"hidden_2/b\",\n",
    "        \"output/W\", \"output/b\"\n",
    "    ]\n",
    "    var_list_opt = [tf.get_variable(v) for v in var_list_opt]\n",
    "    saver.restore(sess, \"frozen_mlp_checkpoint/model-checkpoint-550000\")\n",
    "\n",
    "with tf.variable_scope(\"mlp_init\") as scope:\n",
    "    output_rand = inference(x)\n",
    "    cost_rand = loss(output_rand, y)\n",
    "    scope.reuse_variables()\n",
    "    var_list_rand = [\n",
    "        \"hidden_1/W\", \"hidden_1/b\",\n",
    "        \"hidden_2/W\", \"hidden_2/b\",\n",
    "        \"output/W\", \"output/b\"\n",
    "    ]\n",
    "    var_list_rand = [tf.get_variable(v) for v in var_list_rand]\n",
    "    init_op = tf.variables_initializer(var_list_rand)\n",
    "    sess.run(init_op)\n",
    "\n",
    "with tf.variable_scope(\"mlp_inter\") as scope:\n",
    "    alpha = tf.placeholder(\"float\", [1, 1])\n",
    "    beta = 1 - alpha\n",
    "    h1_W_inter = var_list_opt[0] * beta + var_list_rand[0] * alpha\n",
    "    h1_b_inter = var_list_opt[1] * beta + var_list_rand[1] * alpha\n",
    "    h2_W_inter = var_list_opt[2] * beta + var_list_rand[2] * alpha\n",
    "    h2_b_inter = var_list_opt[3] * beta + var_list_rand[3] * alpha\n",
    "    o_W_inter = var_list_opt[4] * beta + var_list_rand[4] * alpha\n",
    "    o_b_inter = var_list_opt[5] * beta + var_list_rand[5] * alpha\n",
    "    h1_inter = tf.nn.relu(tf.matmul(x, h1_W_inter) + h1_b_inter)\n",
    "    h2_inter = tf.nn.relu(tf.matmul(h1_inter, h2_W_inter) + h2_b_inter)\n",
    "    o_inter = tf.nn.relu(tf.matmul(h2_inter, o_W_inter) + o_b_inter)\n",
    "    cost_inter = loss(o_inter, y)\n",
    "    tf.summary.scalar(\"cost\", cost_inter)\n",
    "\n",
    "summary_writer = tf.summary.FileWriter(\"linear_interp_logs\", graph_def=sess.graph_def)\n",
    "summary_op = tf.summary.merge_all()\n",
    "results = []\n",
    "for a in np.arange(-2, 2, 0.1):\n",
    "    feed_dict = {\n",
    "        x: mnist.test.images,\n",
    "        y: mnist.test.labels,\n",
    "        alpha: [[a]],\n",
    "    }\n",
    "    cost, summary_str = sess.run([cost_inter, summary_op], feed_dict=feed_dict)\n",
    "    summary_writer.add_summary(summary_str, (a + 2)/0.1)\n",
    "    results.append(cost)\n",
    "\n",
    "plt.plot(np.arange(-2, 2, 0.1), results, \"ro\")\n",
    "plt.grid()\n",
    "plt.ylabel(\"error\")\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
